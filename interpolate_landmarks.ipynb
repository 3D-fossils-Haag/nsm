{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate known ladnmarks from mean/median specimen to novel/generated meshes\n",
    "---\n",
    "*Last edited 4 Nov 2025 by K. Wolcott*   \n",
    "Use this notebook to interpret how model is interpreting morphology and landmark positions by invesitgating how landmarks are transferred onto novel/generated meshes from randomly sampled latents.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory for latent codes and model checkpoint files\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # Optional TO DO: Set which GPU to use (0 or 1)\n",
    "\n",
    "# Set working directory\n",
    "TRAIN_DIR = \"run_v51\" # TO DO: Choose training directory containing model ckpt and latent codes\n",
    "cwd = os.getcwd()\n",
    "if TRAIN_DIR not in cwd:\n",
    "    os.chdir(cwd + '/' + TRAIN_DIR)\n",
    "\n",
    "# Build model checkpoint and latent code paths\n",
    "CKPT = '1500' # TO DO: Choose the ckpt value you want to analyze results for\n",
    "LC_PATH = 'latent_codes' + '/' + CKPT + '.pth'\n",
    "MODEL_PATH = 'model' + '/' + CKPT + '.pth'\n",
    "\n",
    "print(\"\\033[92mWorking directory set to: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "from NSM.mesh.interpolate import interpolate_points, interpolate_mesh, interpolate_common\n",
    "from NSM.datasets.sdf_dataset import get_pts_center_and_scale\n",
    "from NSM.mesh import create_mesh\n",
    "from NSM.models import TriplanarDecoder\n",
    "from scipy.spatial import cKDTree\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pyvista as pv\n",
    "import vtk\n",
    "import open3d as o3d\n",
    "from itkwidgets import view\n",
    "\n",
    "import pymskt as mskt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# For setting up icp transform in mesh generation\n",
    "class NumpyTransform:\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = matrix\n",
    "    def GetMatrix(self):\n",
    "        vtk_mat = vtk.vtkMatrix4x4()\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                vtk_mat.SetElement(i, j, self.matrix[i, j])\n",
    "        return vtk_mat\n",
    "\n",
    "# Load model config file\n",
    "def load_config(config_path='model_params_config.json'):\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"\\033[92mLoaded config from {config_path}\\033[0m\")\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: model_params_config.json not found at {config_path}\")\n",
    "\n",
    "# Load trained model and latents\n",
    "def load_model_and_latents(MODEL_PATH, LC_PATH, config, device):\n",
    "    # Load model\n",
    "    triplane_args = {\n",
    "        'latent_dim': config['latent_size'],\n",
    "        'n_objects': config['objects_per_decoder'],\n",
    "        'conv_hidden_dims': config['conv_hidden_dims'],\n",
    "        'conv_deep_image_size': config['conv_deep_image_size'],\n",
    "        'conv_norm': config['conv_norm'], \n",
    "        'conv_norm_type': config['conv_norm_type'],\n",
    "        'conv_start_with_mlp': config['conv_start_with_mlp'],\n",
    "        'sdf_latent_size': config['sdf_latent_size'],\n",
    "        'sdf_hidden_dims': config['sdf_hidden_dims'],\n",
    "        'sdf_weight_norm': config['weight_norm'],\n",
    "        'sdf_final_activation': config['final_activation'],\n",
    "        'sdf_activation': config['activation'],\n",
    "        'sdf_dropout_prob': config['dropout_prob'],\n",
    "        'sum_sdf_features': config['sum_conv_output_features'],\n",
    "        'conv_pred_sdf': config['conv_pred_sdf'],\n",
    "    }\n",
    "    model = TriplanarDecoder(**triplane_args)\n",
    "    model_ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "    model.load_state_dict(model_ckpt['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Load latents\n",
    "    latent_ckpt = torch.load(LC_PATH, map_location=device)\n",
    "    latent_codes = latent_ckpt['latent_codes']['weight'].detach().cpu()\n",
    "    return model, latent_ckpt, latent_codes\n",
    "\n",
    "# Load landmarks file (.mrk.json)\n",
    "def load_mrk_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    if \"markups\" not in data or len(data[\"markups\"]) == 0:\n",
    "        raise ValueError(f\"No 'markups' found in {path}\")\n",
    "    markups = data[\"markups\"][0]\n",
    "    points = []\n",
    "    labels = []\n",
    "    for cp in markups.get(\"controlPoints\", []):\n",
    "        pos = cp.get(\"position\", None)\n",
    "        if pos is not None:\n",
    "            points.append(pos)\n",
    "            labels.append(cp.get(\"label\", None))\n",
    "    points = np.array(points, dtype=np.float32)\n",
    "    return points, labels\n",
    "\n",
    "# Convert vtk mesh to open3d\n",
    "def vtk_to_o3d(vtk_file):\n",
    "    # Load the VTK file using PyVista\n",
    "    vtk_mesh = pv.read(vtk_file)\n",
    "    # Extract vertices and faces\n",
    "    vertices = vtk_mesh.points\n",
    "    faces = vtk_mesh.faces.reshape((-1, 4))[:, 1:]  # Reshape faces to get rid of the first number\n",
    "    # Create Open3D TriangleMesh from the extracted vertices and faces\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    # Optionally, check if normals exist in the VTK file\n",
    "    if vtk_mesh.point_data.get(\"Normals\") is not None:\n",
    "        mesh.vertex_normals = o3d.utility.Vector3dVector(vtk_mesh.point_data[\"Normals\"])\n",
    "    return mesh\n",
    "\n",
    "# Snap landmarks to surface using nearest neighbor\n",
    "def project_landmarks_to_surface(landmarks, target_mesh):\n",
    "    mesh_points = np.asarray(target_mesh.points)\n",
    "    kdtree = cKDTree(mesh_points)\n",
    "    _, idx = kdtree.query(landmarks)\n",
    "    projected = mesh_points[idx]\n",
    "    return projected\n",
    "\n",
    "# Show original mesh and landmarks and generated mesh with interpolated landmarks in external, interactive window\n",
    "def visualize_landmark_interpolation_external(original_mesh, mesh, original_pts, interp_pts, labels=None,\n",
    "                                                animate=False, output_path=None, n_frames=120, off_screen=False, plot_title=\"Landmark Interpolation\"):\n",
    "    # Unwrap MeshWrapper if needed\n",
    "    if hasattr(mesh, \"mesh\"):\n",
    "        mesh = mesh.mesh\n",
    "    # Ensure both meshes are PyVista PolyData\n",
    "    mesh_pv = mesh if isinstance(mesh, pv.PolyData) else mesh.extract_geometry()\n",
    "    original_mesh_pv = (original_mesh if isinstance(original_mesh, pv.PolyData) else original_mesh.extract_geometry())\n",
    "    # Compute normals for smoother rendering\n",
    "    mesh_pv = mesh_pv.compute_normals(cell_normals=False, point_normals=True)\n",
    "    original_mesh_pv = original_mesh_pv.compute_normals(cell_normals=False, point_normals=True)\n",
    "    # Landmarks as PolyData\n",
    "    orig = pv.PolyData(original_pts)\n",
    "    interp = pv.PolyData(interp_pts)\n",
    "    # Lines connecting original → interpolated\n",
    "    lines_polydata = pv.MultiBlock([pv.Line(original_pts[i], interp_pts[i]) for i in range(len(original_pts))]).combine()\n",
    "    # Open Interactive window\n",
    "    plotter = pv.Plotter(window_size=(1024, 768), off_screen=off_screen, notebook=False,)\n",
    "    plotter.add_text(plot_title, font_size=12)\n",
    "    # Add meshes and landmarks\n",
    "    plotter.add_mesh(original_mesh_pv, color=\"lightgray\", opacity=0.4, smooth_shading=True, show_edges=False)\n",
    "    plotter.add_mesh(mesh_pv, color=\"blue\", opacity=0.4, smooth_shading=True, show_edges=False)\n",
    "    plotter.add_mesh(orig, color=\"red\", point_size=14, render_points_as_spheres=True)\n",
    "    plotter.add_mesh(interp, color=\"green\", point_size=14, render_points_as_spheres=True)\n",
    "    plotter.add_mesh(lines_polydata, color=\"orange\", line_width=2)\n",
    "    # Optional labels\n",
    "    if labels is not None:\n",
    "        for pt, label in zip(original_pts, labels):\n",
    "            plotter.add_point_labels([pt], [label], font_size=10, text_color=\"black\")\n",
    "    # Show in a standalone GUI window\n",
    "    plotter.show_axes()\n",
    "    plotter.show_grid()\n",
    "    if not animate:\n",
    "        plotter.show(interactive=True, auto_close=True)\n",
    "    # Save animated path (rotating meshes with LMs)\n",
    "    if animate:\n",
    "        if output_path is None:\n",
    "            raise ValueError(\"output_path must be provided when animate=True\")\n",
    "        if output_path.suffix.lower() == \".gif\":\n",
    "            plotter.open_gif(str(output_path))\n",
    "        else:\n",
    "            plotter.open_movie(str(output_path), framerate=30, quality=9)\n",
    "        # Initialize camera once for reproducibility\n",
    "        plotter.camera.zoom(1.2)\n",
    "        for _ in range(n_frames):\n",
    "            plotter.camera.azimuth += 360 / n_frames\n",
    "            plotter.write_frame()\n",
    "        plotter.close()\n",
    "        return\n",
    "\n",
    "# Show set of landmarks on a mesh\n",
    "def visualize_landmarks_on_mesh(original_mesh, original_pts):\n",
    "    # Ensure both meshes are PyVista PolyData\n",
    "    original_mesh_pv = original_mesh if isinstance(original_mesh, pv.PolyData) else original_mesh.extract_geometry()\n",
    "    # Compute normals for smoother rendering\n",
    "    original_mesh_pv = original_mesh_pv.compute_normals(cell_normals=False, point_normals=True, inplace=False)\n",
    "    # Landmarks as PolyData\n",
    "    orig = pv.PolyData(original_pts)\n",
    "    # Open Interactive window\n",
    "    plotter = pv.Plotter(window_size=[1024, 768], notebook=False, off_screen=False)\n",
    "    plotter.add_text(\"Landmark Interpolation\", font_size=12)\n",
    "    # Add meshes and landmarks\n",
    "    plotter.add_mesh(original_mesh_pv, color=\"lightgray\", opacity=0.4, smooth_shading=True, show_edges=False)\n",
    "    plotter.add_mesh(orig, color=\"red\", point_size=14, render_points_as_spheres=True)\n",
    "    plotter.show_axes()\n",
    "    plotter.show_grid()\n",
    "    # Show in a standalone GUI window\n",
    "    plotter.show(interactive=True, auto_close=True)\n",
    "\n",
    "# Normalize mesh to unit sphere to match NSM backend scaling\n",
    "def normalize_mesh_to_unit_sphere(mesh, reference_points=None):\n",
    "    # Extract vertices\n",
    "    if isinstance(mesh, pv.PolyData):\n",
    "        vertices = mesh.points.copy()\n",
    "    else:  # assume numpy array of shape (N, 3)\n",
    "        vertices = mesh.copy()\n",
    "    if reference_points is None:\n",
    "        reference_points = vertices\n",
    "    # Compute center and scale\n",
    "    center = np.mean(reference_points, axis=0)\n",
    "    shifted = vertices - center\n",
    "    scale = np.max(np.linalg.norm(reference_points - center, axis=1))\n",
    "    normalized_vertices = shifted / scale\n",
    "    # If PyVista mesh, return new mesh\n",
    "    if isinstance(mesh, pv.PolyData):\n",
    "        normalized_mesh = mesh.copy()\n",
    "        normalized_mesh.points = normalized_vertices\n",
    "        return normalized_mesh, center, scale\n",
    "    else:\n",
    "        return normalized_vertices, center, scale\n",
    "\n",
    "# Convert open3d to pyvista mesh    \n",
    "def o3d_to_pv(mesh_o3d):\n",
    "    # Get vertices and faces as numpy arrays\n",
    "    vertices = np.asarray(mesh_o3d.vertices)\n",
    "    faces = np.asarray(mesh_o3d.triangles)\n",
    "    # PyVista expects faces in a flattened format: [3, v0, v1, v2, 3, ...]\n",
    "    faces_flat = np.hstack([np.full((faces.shape[0], 1), 3), faces]).flatten()\n",
    "    # Create PyVista mesh\n",
    "    mesh_pv = pv.PolyData(vertices, faces_flat)\n",
    "    return mesh_pv\n",
    "\n",
    "# Save landmark file for rendering in 3D slicer (.mrk.json)\n",
    "def save_mrk_json(points, labels, output_filename):\n",
    "    control_points = []\n",
    "    for i, (pt, label) in enumerate(zip(points, labels), start=1):\n",
    "        # Ensure pt is a list or numpy array of coordinates\n",
    "        if isinstance(pt, np.ndarray):\n",
    "            pt = pt.tolist()  # If pt is a numpy array, convert to a list\n",
    "        elif isinstance(pt, list):\n",
    "            pass  # pt is already a list, no conversion needed\n",
    "        else:\n",
    "            raise ValueError(f\"Expected pt to be a list or numpy array, got {type(pt)}\")\n",
    "        cp = {\n",
    "            \"id\": str(i),\n",
    "            \"label\": label,\n",
    "            \"description\": \"\",\n",
    "            \"associatedNodeID\": \"\",\n",
    "            \"position\": pt,  # No need to call .tolist() here if it's already a list\n",
    "            \"orientation\": [-1.0, -0.0, -0.0, -0.0, -1.0, -0.0, 0.0, 0.0, 1.0],\n",
    "            \"selected\": True,\n",
    "            \"locked\": True,  # Lock the position so it can't be moved in Slicer\n",
    "            \"lockedPosition\": True,  # Optional: If you want explicit \"lockedPosition\" field\n",
    "            \"visibility\": True,\n",
    "            \"positionStatus\": \"defined\"}  # Ensure the position is defined and not undefined\n",
    "        control_points.append(cp)\n",
    "    markups_data = {\n",
    "        \"@schema\": \"https://raw.githubusercontent.com/slicer/slicer/master/Modules/Loadable/Markups/Resources/Schema/markups-schema-v1.0.3.json#\",\n",
    "        \"markups\": [{\n",
    "                \"type\": \"Fiducial\",\n",
    "                \"coordinateSystem\": \"LPS\",\n",
    "                \"coordinateUnits\": \"mm\",\n",
    "                \"locked\": False,  # Keep the overall markup locked status as False for editing purposes\n",
    "                \"fixedNumberOfControlPoints\": False,\n",
    "                \"labelFormat\": \"%N-%d\",\n",
    "                \"lastUsedControlPointNumber\": len(control_points),\n",
    "                \"controlPoints\": control_points,\n",
    "                \"measurements\": [],\n",
    "                \"display\": {\n",
    "                    \"visibility\": False,\n",
    "                    \"opacity\": 1.0,\n",
    "                    \"color\": [0.4, 1.0, 1.0],\n",
    "                    \"selectedColor\": [1.0, 0.5000076295109483, 0.5000076295109483],\n",
    "                    \"activeColor\": [0.4, 1.0, 0.0],\n",
    "                    \"propertiesLabelVisibility\": False,\n",
    "                    \"pointLabelsVisibility\": False,\n",
    "                    \"textScale\": 3.0,\n",
    "                    \"glyphType\": \"Sphere3D\",\n",
    "                    \"glyphScale\": 3.0,\n",
    "                    \"glyphSize\": 5.0,\n",
    "                    \"useGlyphScale\": True,\n",
    "                    \"sliceProjection\": False,\n",
    "                    \"sliceProjectionUseFiducialColor\": True,\n",
    "                    \"sliceProjectionOutlinedBehindSlicePlane\": False,\n",
    "                    \"sliceProjectionColor\": [1.0, 1.0, 1.0],\n",
    "                    \"sliceProjectionOpacity\": 0.6,\n",
    "                    \"lineThickness\": 0.2,\n",
    "                    \"lineColorFadingStart\": 1.0,\n",
    "                    \"lineColorFadingEnd\": 10.0,\n",
    "                    \"lineColorFadingSaturation\": 1.0,\n",
    "                    \"lineColorFadingHueOffset\": 0.0,\n",
    "                    \"handlesInteractive\": False,\n",
    "                    \"translationHandleVisibility\": True,\n",
    "                    \"rotationHandleVisibility\": True,\n",
    "                    \"scaleHandleVisibility\": False,\n",
    "                    \"interactionHandleScale\": 3.0,\n",
    "                    \"snapMode\": \"toVisibleSurface\"}}]}\n",
    "    # Save to JSON file\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(markups_data, f, indent=4)\n",
    "    print(f\"Saved to {output_filename}\")\n",
    "\n",
    "# Interpolate landmark points from latent 1 to latent 2\n",
    "def interpolate_points(\n",
    "    model, latent1, latent2, n_steps=100, points1=None, surface_idx=0, verbose=False, spherical=True, smooth_type=\"taubin\"):\n",
    "    return interpolate_common(\n",
    "        model, latent1, latent2, n_steps, points1, surface_idx, verbose, spherical, is_mesh=False, smooth_type=\"taubin\")\n",
    "\n",
    "# Get indices for interpolation by filename or randomly\n",
    "def get_indices(train_paths, random_sample=True, n_meshes=5, string_to_match=None):\n",
    "    # If randomly sampling\n",
    "    if random_sample == True and string_to_match == None:\n",
    "        if string_to_match is not None:\n",
    "            raise ValueError(\"If randomly sampling, string_to_match will not be considered. If matching by string, random_sample must be set to False.\")\n",
    "        print(f\"Randomly sampling {n_meshes} indices from latent codes\")\n",
    "        indices = np.random.randint(0, len(train_paths), n_meshes)\n",
    "    # If matching by string\n",
    "    else:\n",
    "        if string_to_match is None:\n",
    "            raise ValueError(\"Target string must be provided when not sampling randomly.\")\n",
    "        print(f\"Finding {n_meshes} latent code indices based on filenames containing: '{string_to_match}'\")\n",
    "        matched_indices = [i for i, s in enumerate(train_paths) if string_to_match.lower() in s.lower()]\n",
    "        if len(matched_indices) < n_meshes:\n",
    "            print(f\"Warning: Only {len(matched_indices)} matches found, sampling all.\")\n",
    "            # Return all matching indices if not enough for n_meshes\n",
    "            indices = np.array(matched_indices)\n",
    "        else:\n",
    "            indices = np.random.choice(matched_indices, n_meshes, replace=False)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model loading and mesh creation parameters\n",
    "\n",
    "# Load model config\n",
    "config = load_config()\n",
    "device = config.get(\"device\", \"cuda:0\")\n",
    "train_paths = config['list_mesh_paths']\n",
    "#train_paths = config['test_paths']\n",
    "all_vtk_files = [os.path.basename(f) for f in train_paths]\n",
    "\n",
    "# Load model and latent codes\n",
    "model, latent_ckpt, latent_codes = load_model_and_latents(MODEL_PATH, LC_PATH, config, device)\n",
    "\n",
    "# Mesh creation params\n",
    "recon_grid_origin = 1.0\n",
    "n_pts_per_axis = 128\n",
    "voxel_origin = (-recon_grid_origin, -recon_grid_origin, -recon_grid_origin)\n",
    "voxel_size = (recon_grid_origin * 2) / (n_pts_per_axis - 1)\n",
    "offset = np.array([0.0, 0.0, 0.0])\n",
    "scale = 1.0\n",
    "icp_transform = NumpyTransform(np.eye(4))\n",
    "objects = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the specimen closest to the median, and use that mesh and landmarks as a template\n",
    "\n",
    "# Compute median latent\n",
    "latent_codes = latent_codes.to(device)\n",
    "median_latent = torch.median(latent_codes, dim=0).values  # shape: (latent_dim,)\n",
    "\n",
    "# Normalize both sets of vectors to unit length\n",
    "latent_norm = F.normalize(latent_codes, p=2, dim=1)\n",
    "median_latent_norm = F.normalize(median_latent.unsqueeze(0), p=2, dim=1)\n",
    "\n",
    "# Get cosine similarity and distance\n",
    "cosine_similarities = torch.mm(latent_norm, median_latent_norm.T).squeeze()\n",
    "cosine_distances = 1 - cosine_similarities\n",
    "\n",
    "# Find specimen closest to median latent\n",
    "med_idx = torch.argmin(cosine_distances).item()\n",
    "print(f\"Closest specimen to median latent: {all_vtk_files[med_idx]}\")\n",
    "med_fn = \"../vertebrae_meshes/\" + all_vtk_files[med_idx]\n",
    "med_lm_fn = '../alignedLMs/' + os.path.splitext(all_vtk_files[med_idx])[0] + '.mrk.json'\n",
    "print(\"Associated landmark file: \", med_lm_fn, \"\\033[0m\\n\")\n",
    "\n",
    "# Save filename to text file\n",
    "interp_dir = Path(\"interpedLMs\")\n",
    "interp_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_txt = interp_dir / \"closest_specimen_to_median.txt\"\n",
    "with open(output_txt, \"w\") as f:\n",
    "    f.write(f\"Mesh: {med_fn}\\n\")\n",
    "    f.write(f\"Landmarks: {med_lm_fn}\\n\")\n",
    "print(f\"Saved closest specimen to median info to :{output_txt}\")\n",
    "\n",
    "# Prep mesh and landmarks for compatibility with NSM backend\n",
    "med_mesh = vtk_to_o3d(med_fn)\n",
    "med_mesh = o3d_to_pv(med_mesh)\n",
    "med_lms, med_labels = load_mrk_json(med_lm_fn)\n",
    "median_mesh, mesh_center, mesh_scale = normalize_mesh_to_unit_sphere(med_mesh)\n",
    "normalized_lms = (med_lms - mesh_center) / mesh_scale\n",
    "median_lms = project_landmarks_to_surface(normalized_lms, median_mesh.mesh if hasattr(median_mesh, \"mesh\") else median_mesh)\n",
    "\n",
    "# Visualize landmarks on mesh in external viewer\n",
    "visualize_landmarks_on_mesh(median_mesh, median_lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate points from median specimen to random specimens\n",
    "\n",
    "# Save resulting landmarks and meshes?\n",
    "save_results = False # TO DO: Set to True or False\n",
    "\n",
    "# Get indices of latents to interpolate\n",
    "indices = get_indices(train_paths=all_vtk_files, random_sample=True, n_meshes=5, string_to_match=None)\n",
    "\n",
    "# Main interpolation\n",
    "interpolated_points = []\n",
    "meshes = []\n",
    "mesh_names = []\n",
    "\n",
    "# Loop through randomly selected latent codes for interpolation\n",
    "for i, idx in enumerate(indices):\n",
    "    # Set out filename for landmarks\n",
    "    vtk_path = Path(all_vtk_files[idx])\n",
    "    stem = vtk_path.stem\n",
    "\n",
    "    print(f\"\\n\\033[92m{i}: {vtk_path.name}\\033[0m\")\n",
    "    mesh_names.append((idx, vtk_path.name))\n",
    "\n",
    "    out_lm_fn = interp_dir / f\"interped_{stem}.mrk.json\"\n",
    "    out_mesh_fn = interp_dir / f\"interped_{stem}.vtk\"\n",
    "    # Load latent codes by index\n",
    "    latent = latent_codes[idx]\n",
    "    # Create mesh from latent\n",
    "    mesh_out = create_mesh(\n",
    "            decoder=model, latent_vector=latent, n_pts_per_axis=n_pts_per_axis,\n",
    "            voxel_origin=voxel_origin, voxel_size=voxel_size, path_original_mesh=None,\n",
    "            offset=offset, scale=scale, icp_transform=icp_transform,\n",
    "            objects=objects, verbose=False, device=device\n",
    "        )\n",
    "    mesh_out = mesh_out[0] if isinstance(mesh_out, list) else mesh_out\n",
    "    mesh_out.resample_surface(clusters=20_000)\n",
    "\n",
    "    # Normalize latent vectors\n",
    "    latent1 = median_latent.cpu().numpy().astype(np.float64)\n",
    "    latent2 = latent.cpu().numpy().astype(np.float64)\n",
    "\n",
    "    # --- Interpolate landmarks (these are now in normalized space) ---\n",
    "    interp_pts = interpolate_points(\n",
    "        model=model,\n",
    "        latent1=latent1,\n",
    "        latent2=latent2,\n",
    "        points1=median_lms,\n",
    "        n_steps=100,\n",
    "        smooth_type=\"taubin\"\n",
    "    )\n",
    "    \n",
    "    # Normalize mesh and landmarks to match NSM backend\n",
    "    normalized_mesh, mesh_center, mesh_scale = normalize_mesh_to_unit_sphere(mesh_out)\n",
    "    normalized_lms = (interp_pts - mesh_center) / mesh_scale\n",
    "    \n",
    "    # Append generated mesh and interpolated points to list\n",
    "    meshes.append(normalized_mesh)\n",
    "    interpolated_points.append(normalized_lms)\n",
    "\n",
    "    # Optional: Save interpolated landmarks and generated meshes\n",
    "    if save_results:\n",
    "        print(\"Saving results for interpolated landmarks and meshes...\")\n",
    "        save_mrk_json(normalized_lms, med_labels, out_lm_fn)\n",
    "        orig_lm_fn = Path(\"..\") / \"alignedLMs\" / f\"{stem}.mrk.json\"\n",
    "        dest_lm_fn = interp_dir / orig_lm_fn.name\n",
    "        shutil.copy(orig_lm_fn, dest_lm_fn)\n",
    "        normalized_mesh.save(out_mesh_fn)\n",
    "        print(f\"Saved mesh → {out_mesh_fn.name}\")\n",
    "        print(f\"Copied original landmarks → {dest_lm_fn.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interpolated landmarks and generated mesh in interactive viewer\n",
    "pv.set_jupyter_backend(None)  # disable notebook mode completely\n",
    "\n",
    "# Pick index to inspect\n",
    "idx_to_show = 1 # change index as needed\n",
    "\n",
    "# Get corresponding mesh name\n",
    "mesh_name = mesh_names[idx_to_show][1]  # assuming mesh_names stores (idx, name)\n",
    "plot_title = \"Median specimen: \" + all_vtk_files[med_idx] + \"\\n vs \\n\" + \"Interpolated specimen: \" + mesh_name\n",
    "\n",
    "# Show results in external viewer\n",
    "visualize_landmark_interpolation_external(\n",
    "    median_mesh, # original mesh\n",
    "    meshes[idx_to_show], # generated mesh\n",
    "    median_lms,  # original landmarks\n",
    "    interpolated_points[idx_to_show],  # interpolated landmarks\n",
    "    med_labels,  # labels\n",
    "    plot_title = plot_title,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate interpolated landmarks and generated mesh to gif\n",
    "\n",
    "# Pick index to inspect\n",
    "idx_to_show = 0  # change index as needed\n",
    "\n",
    "# Get corresponding mesh name\n",
    "mesh_name = mesh_names[idx_to_show][1]  # assuming mesh_names stores (idx, name)\n",
    "plot_title = \"Median specimen: \" + all_vtk_files[med_idx] + \"\\n vs \\n\" + \"Interpolated specimen: \" + mesh_name\n",
    "\n",
    "# Create GIF directory\n",
    "gif_dir = interp_dir / \"gifs\"\n",
    "gif_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct output path safely\n",
    "output_fname = f\"{idx_to_show}_{mesh_name}.gif\"\n",
    "output_fpath = gif_dir / output_fname\n",
    "\n",
    "visualize_landmark_interpolation_external(\n",
    "    median_mesh,\n",
    "    meshes[idx_to_show],\n",
    "    median_lms,\n",
    "    interpolated_points[idx_to_show],\n",
    "    med_labels,\n",
    "    animate=True,\n",
    "    output_path=output_fpath,\n",
    "    off_screen=True,\n",
    "    n_frames=240,\n",
    "    plot_title = plot_title,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# Utility functions for loading trained models and inspecting results

import os, json, torch, numpy as np, open3d as o3d, pyvista as pv, vtk
from NSM.models import TriplanarDecoder
from scipy.spatial import cKDTree
from sklearn.decomposition import PCA
import re
import pymskt.mesh.meshes as meshes
import torch.nn.functional as F
import cv2
from NSM.mesh import create_mesh

# ICP transform
class NumpyTransform:
    def __init__(self, matrix):
        self.matrix = matrix
    def GetMatrix(self):
        vtk_mat = vtk.vtkMatrix4x4()
        for i in range(4):
            for j in range(4):
                vtk_mat.SetElement(i, j, self.matrix[i, j])
        return vtk_mat

# Pyvista to Open3D    
def pv_to_o3d(mesh_pv):
    pts = np.asarray(mesh_pv.points)
    faces = np.asarray(mesh_pv.faces)
    tris = faces.reshape(-1,4)[:,1:4]
    mesh_o3d = o3d.geometry.TriangleMesh()
    mesh_o3d.vertices = o3d.utility.Vector3dVector(pts)
    mesh_o3d.triangles = o3d.utility.Vector3iVector(tris)
    mesh_o3d.compute_vertex_normals()
    return mesh_o3d

# Convert ply file to vtk
def convert_ply_to_vtk(input_file, output_file=None, save=False):
    if not input_file.lower().endswith('.ply'):
        raise ValueError("Input file must have a .ply extension.")
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"Input file does not exist: {input_file}")
    if output_file is None:
        output_file = os.path.splitext(input_file)[0] + ".vtk"
    mesh = pv.read(input_file)
    if save==True:
        mesh.save(output_file)
    print(f"Converted {input_file} → {output_file}")
    return mesh, output_file

# Load model config file
def load_config(config_path='model_params_config.json'):
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"\033[92mLoaded config from {config_path}\033[0m")
        return config
    except FileNotFoundError:
        raise FileNotFoundError(f"Error: model_params_config.json not found at {config_path}")

# Load model and latents
def load_model_and_latents(MODEL_PATH, LC_PATH, config, device):
    # Load model
    triplane_args = {
        'latent_dim': config['latent_size'],
        'n_objects': config['objects_per_decoder'],
        'conv_hidden_dims': config['conv_hidden_dims'],
        'conv_deep_image_size': config['conv_deep_image_size'],
        'conv_norm': config['conv_norm'], 
        'conv_norm_type': config['conv_norm_type'],
        'conv_start_with_mlp': config['conv_start_with_mlp'],
        'sdf_latent_size': config['sdf_latent_size'],
        'sdf_hidden_dims': config['sdf_hidden_dims'],
        'sdf_weight_norm': config['weight_norm'],
        'sdf_final_activation': config['final_activation'],
        'sdf_activation': config['activation'],
        'sdf_dropout_prob': config['dropout_prob'],
        'sum_sdf_features': config['sum_conv_output_features'],
        'conv_pred_sdf': config['conv_pred_sdf'],
    }
    model = TriplanarDecoder(**triplane_args)
    model_ckpt = torch.load(MODEL_PATH, map_location=device)
    model.load_state_dict(model_ckpt['model'])
    model.to(device)
    model.eval()
    # Load latents
    latent_ckpt = torch.load(LC_PATH, map_location=device)
    latent_codes = latent_ckpt['latent_codes']['weight'].detach().cpu()
    return model, latent_ckpt, latent_codes

# begin monkey patch
def safe_load_mesh_scalars(self):
    try:
        if hasattr(self, 'mesh'):
            mesh = self.mesh
        elif hasattr(self, '_mesh'):
            mesh = self._mesh
        else:
            raise AttributeError("No mesh attribute found in Mesh object.")
        point_scalars = mesh.point_data
        cell_scalars = mesh.cell_data
        if point_scalars and len(point_scalars.keys()) > 0:
            self.mesh_scalar_names = list(point_scalars.keys())
            self.scalar_name = self.mesh_scalar_names[0]
        elif cell_scalars and len(cell_scalars.keys()) > 0:
            self.mesh_scalar_names = list(cell_scalars.keys())
            self.scalar_name = self.mesh_scalar_names[0]
        else:
            self.mesh_scalar_names = []
            self.scalar_name = None
            print("No scalar data found in mesh. Proceeding without scalars.")
    except Exception as e:
        print(f"Failed to load mesh scalars: {e}")
        self.mesh_scalar_names = []
        self.scalar_name = None

def fixed_point_coords(self):
    if self.n_points < 1:
        raise AttributeError(f"No points found in mesh '{self}'")
    return self.points

def get_sdfs(decoder, samples, latent_vector, batch_size=32**3, objects=1, device='cuda'):
    n_pts_total = samples.shape[0]
    current_idx = 0
    sdf_values = torch.zeros(samples.shape[0], objects, device=device) 
    if batch_size > n_pts_total:
        #print('WARNING: batch_size is greater than the number of samples, setting batch_size to the number of samples')
        batch_size = n_pts_total
    while current_idx < n_pts_total:
        current_batch_size = min(batch_size, n_pts_total - current_idx)
        sampled_pts = samples[current_idx : current_idx + current_batch_size, :3].to(device)
        sdf_values[current_idx : current_idx + current_batch_size, :] = decode_sdf(
            decoder, latent_vector, sampled_pts, device) 
        current_idx += current_batch_size
        #print(f"Processed {current_idx} / {n_pts_total} points")
    return sdf_values

def decode_sdf(decoder, latent_vector, queries, device='cuda'):
    num_samples = queries.shape[0]
    if latent_vector is None:
        inputs = queries
    else:
        latent_repeat = latent_vector.expand(num_samples, -1).to(device)
        inputs = torch.cat([latent_repeat, queries], dim=1)
    inputs = inputs.to(next(decoder.parameters()).device)  
    return decoder(inputs)
# end monkey patch

# Get species name using regex from filenames (ex: Scincidae_Tribolonotus_novaeguineae)
def extract_species_prefix(filename):
    match = re.match(r"([A-Za-z]+_[A-Za-z]+_[a-z]+)", filename.lower())
    if match:
        return match.group(1)
    else:
        return None
    
def average_across_regions(regex_pattern, vert_region, vert_region_files, vert_region_codes):
    specimen_pattern = re.compile(regex_pattern, re.IGNORECASE)
    specimen_latents = {}
    specimen_files = {}
    # Group latents by region for averaging using regex
    for fname, latent in zip(vert_region_files, vert_region_codes):
        match = specimen_pattern.match(fname)
        if match:
            specimen_id = match.group(1)
            if specimen_id not in specimen_latents:
                specimen_latents[specimen_id] = []
                specimen_files[specimen_id] = []
            specimen_latents[specimen_id].append(latent.numpy())
            specimen_files[specimen_id].append(fname)
        else:
            print(f"\033[93mWarning: could not extract specimen ID from {fname}\033[0m")
    # Average the latent codes per specimen
    avg_latent_codes = []
    avg_specimen_ids = []
    for specimen_id, latents in specimen_latents.items():
        avg_latent = np.mean(latents, axis=0)
        avg_latent_codes.append(avg_latent)
        avg_specimen_ids.append(specimen_id + '_' + vert_region)
    # Convert to NumPy array
    avg_latent_codes = np.array(avg_latent_codes)
    print(f"\nAveraged latent codes for {len(avg_specimen_ids)} specimens.\nSample specimen IDs: {avg_specimen_ids[:5]}")
    vert_region_codes = avg_latent_codes
    vert_region_files = avg_specimen_ids
    return vert_region_files, vert_region_codes

# Overlay specimen name info onto each frame
def overlay_text_on_frame(frame, i, loop_sequence_names):
    specimen_name = loop_sequence_names[i]
    parts = specimen_name.split("_")
    family = parts[0] if len(parts) > 0 else specimen_name
    genus = parts[1]  if len(parts) > 1 else ""
    region = parts[-1] if len(parts) > 2 else ""
    if 'C' in region:
        reg_full = 'Cervical'
    elif 'T' in region:
        reg_full = 'Thoracic'
    elif 'L' in region:
        reg_full = 'Lumbar'
    else:
        reg_full = ''
    text = f"Closest Specimen: \n{family}\n{genus}\n{reg_full}"
    # Text settings
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.7
    font_color = (255, 255, 255)  # White
    thickness = 2
    text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)
    # Position: center of the frame
    center_x = frame.shape[1] // 2
    center_y = frame.shape[0] // 2
    text_x = center_x - 120
    text_y = center_y
    # Put the text
    for j, line in enumerate(text.split("\n")):
            y = text_y + j * (text_size[1] + 10)
            cv2.putText(frame, line, (text_x, y), font, font_scale, font_color, thickness, cv2.LINE_AA)
    return frame

def render_cameras(renderers, mesh_o3d, i, material, loop_sequence, n_rotations):
    for r in renderers:
        r.scene.clear_geometry()
        r.scene.add_geometry("mesh", mesh_o3d, material)
    # Camera setup
    pts = np.asarray(mesh_o3d.vertices)
    center = pts.mean(axis=0)
    r = np.linalg.norm(pts - center, axis=1).max()
    distance = 2.5 * r
    elevation = np.deg2rad(30)
    # Define 4 camera positions
    angle_deg = (i /  (len(loop_sequence) - 1)) * 360 * n_rotations
    angle_rad = np.deg2rad(angle_deg)
    cam_positions = [center + np.array([  # Top Left: rotating
                    distance * np.cos(angle_rad) * np.cos(elevation),
                    distance * np.sin(angle_rad) * np.cos(elevation),
                    distance * np.sin(elevation)]),
                    center + np.array([0, -distance, 0]),  # Top Right: side
                    center + np.array([distance, 0, 0]),  # Bottom Left: back (90° CCW from side)
                    center + np.array([0, 0, distance])]    # Bottom Right: top-down (90° CCW from side)
    ups = [[0, 0, 1],  # rotating
            [1, 0, 0],  # front
            [0, 0, 1],  # side
            [0, 1, 0],]  # top-down

    # Define other camera positions (side, top-down, etc.)
    for idx, (rdr, pos, up) in enumerate(zip(renderers, cam_positions, ups)):
        rdr.setup_camera(60, center, pos, up)
    # Render images
    imgs = [np.asarray(r.render_to_image()) for r in renderers]
    imgs_bgr = [cv2.cvtColor(img, cv2.COLOR_RGB2BGR) for img in imgs]
    # Compose 4 views into 2x2 grid (width=640, height=480)
    top = np.hstack([imgs_bgr[0], imgs_bgr[1]])
    bottom = np.hstack([imgs_bgr[2], imgs_bgr[3]])
    combined = np.vstack([top, bottom])
    return combined

def generate_and_render_mesh(latent_code, loop_sequence_names, loop_sequence, i, device, model, n_pts_per_axis,
                             voxel_origin, voxel_size, offset, scale, icp_transform, objects, generated_mesh_count):
    generated_mesh_count += 1
    print(f"\033[92m\nGenerating mesh {generated_mesh_count}/{len(loop_sequence)}\033[0m")
    print(f"Frame {i}: Closest to {loop_sequence_names[i]}")
    new_latent = torch.tensor(latent_code, dtype=torch.float32).unsqueeze(0).to(device)

    mesh_out = create_mesh(
            decoder=model, latent_vector=new_latent, n_pts_per_axis=n_pts_per_axis,
            voxel_origin=voxel_origin, voxel_size=voxel_size, path_original_mesh=None,
            offset=offset, scale=scale, icp_transform=icp_transform,
            objects=objects, verbose=False, device=device)
        
    mesh_out = mesh_out[0] if isinstance(mesh_out, list) else mesh_out
    mesh_pv = mesh_out if isinstance(mesh_out, pv.PolyData) else mesh_out.extract_geometry()
    mesh_pv = mesh_pv.compute_normals(cell_normals=False, point_normals=True, inplace=False)
    mesh_o3d = pv_to_o3d(mesh_pv)
    return mesh_o3d
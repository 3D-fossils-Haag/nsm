{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/nsm/blob/main/demos/shape_completion_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuJS_OF3wA8K"
      },
      "source": [
        "#**Squamate Vertebra Shape Completion Demo**   \n",
        "*Last edited 19 Jan 2026*\n",
        "\n",
        "\n",
        "This notebook demonstrates shape completion for partial vertebrae (modern and fossil) using a trained Neural Shape Model (NSM; Gatti et al. 2025, Park et al. 2019). It can be run fully in demo mode without connecting to your Google Drive. Adjust parameters using form fields and make sure your runtime environment is set to run on GPU. Full repository code is available at [aubricot/nsm on GitHub](https://github.com/aubricot/nsm).\n",
        "\n",
        "Modern vertebra meshes are derived from micro-CT data produced by the oVert Initiative (Blackburn et al. 2024). Fossil vertebra were downloaded from MorphoSource ([UF546657](https://doi.org/10.17602/M2/M600663); [UF271967](https://n2t.net/ark:/87602/m4/M69199)). All vertebrae were aligned and scaled using ATLAS before training (Porto et al. 2026).\n",
        "\n",
        "\n",
        "**References**\n",
        "* Blackburn et al. 2024, BioScience. https://doi.org/10.1093/biosci/biad120\n",
        "* Gatti et al. 2025, IEEE TMI. https://doi.org/10.1109/tmi.2024.3485613\n",
        "* Park et al. 2019, CVPR. https://doi.org/10.48550/arXiv.1901.05103\n",
        "* Porto et al. 2026, in prep. https://github.com/agporto/ATLAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arE-9o6jwA8L"
      },
      "source": [
        "# 1. Installs & Imports\n",
        "---\n",
        "This notebook can run fully in the cloud or can be connected to your Google Drive to save results. Run these code blocks to set up your environment before proceeding with shape completion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fYYdToleoKw"
      },
      "source": [
        "## Check that your Colab runtime environment is set to use GPU.\n",
        "Go to the top right corner of this notebook → Click the arrow → Change runtime type  → Hardware Accelerator  → any option with GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX_Hd2ZXibqe"
      },
      "outputs": [],
      "source": [
        "# Check GPU and CUDA info - make sure Colab Runtime set to GPU\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# Check GPU and CUDA\n",
        "!nvcc --version\n",
        "gpu = !nvidia-smi\n",
        "gpu = '\\n'.join(gpu)\n",
        "print('\\033[91mNot connected to a GPU\\033[0m' if 'failed' in gpu else gpu)\n",
        "\n",
        "# Check RAM\n",
        "ram = virtual_memory().total / 1e9\n",
        "print(f'\\033[92mYour runtime has {ram:.1f} GB of RAM\\033[0m\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV8nITDKfnhz"
      },
      "source": [
        "## Choose where to save results\n",
        "This notebook can be run fully within the Colab runtime environment (files are deleted after each session) or it can be connected to your Google Drive. To customize, run the form fields on the right to set the parameters for \"save\" and \"base_wd\" to determine your base working directory where files will go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP-qynnLiqNA"
      },
      "outputs": [],
      "source": [
        "# Choose where to save results\n",
        "\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image tagging file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "base_wd = \"/content/drive/MyDrive\" # @param [\"/content/drive/MyDrive/nsm\"] {\"allow-input\":true}\n",
        "wd = base_wd + \"/nsm\"\n",
        "print(f\"\\033[92mWorking directory set to: \\n{wd}\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72FXs62fgD3m"
      },
      "source": [
        "## Set up environment and install NSM\n",
        "Machine learning environments have specific versions required for certain libaries. The code below first ensures that all the versions installed are compatible with NSM. Then, it clones NSM from GitHub, installs other required packages, and installs NSM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHhleWhCtA1b"
      },
      "outputs": [],
      "source": [
        "# Set up environment and install NSM\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Install PyTorch with CUDA support (Colab typically has CUDA 11.8 or 12.x)\n",
        "print(\"\\033[92mSetting up environment...\\033[0m\")\n",
        "print(\"\\n\\033[33m-----This will take a few minutes----\\033[0m\")\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install pyvista mskt open3d scikit-learn matplotlib pandas numpy scipy\n",
        "!pip install ipywidgets\n",
        "!pip install nibabel scikit-image opencv-python open3d\n",
        "\n",
        "# Clone NSM repository\n",
        "if not os.path.exists(wd):\n",
        "    print(\"Cloning NSM repository...\")\n",
        "    os.makedirs(base_wd, exist_ok=True)\n",
        "    %cd $base_wd\n",
        "    !git clone https://github.com/aubricot/nsm.git\n",
        "else:\n",
        "    print(\"NSM directory already exists\")\n",
        "\n",
        "# Navigate to nsm directory and install\n",
        "%cd $wd\n",
        "\n",
        "# Install requirements\n",
        "print(\"\\n-----Installing requirements-----\")\n",
        "!python -m pip install -r requirements.txt\n",
        "\n",
        "# Install NSM package\n",
        "print(\"\\n-----Installing NSM-----\")\n",
        "!pip install .\n",
        "\n",
        "# Add to Python path\n",
        "sys.path.insert(0, wd)\n",
        "%cd $wd\n",
        "print(f\"\\n\\033[92mCurrent working directory set to: {os.getcwd()}\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1aLKbpJgxui"
      },
      "source": [
        "## Import libraries and define functions\n",
        "Import required libraries and define functions to be used downstream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7teNyyTwCfx"
      },
      "outputs": [],
      "source": [
        "# Import libraries and define functions\n",
        "\n",
        "# For rendering meshes\n",
        "import pyvista as pv\n",
        "pv.start_xvfb() # Enable PyVista for Colab\n",
        "import plotly.graph_objects as go\n",
        "import pymskt.mesh.meshes as meshes\n",
        "\n",
        "# For working with ML\n",
        "import torch\n",
        "from NSM.helper_funcs import load_config, load_model_and_latents\n",
        "from NSM.optimization import get_top_k_pcs\n",
        "from NSM.helper_funcs import NumpyTransform, convert_ply_to_vtk\n",
        "from NSM.optimization import (sample_near_surface,\n",
        "    downsample_partial_pointcloud,\n",
        "    optimize_latent_partial)\n",
        "from NSM.datasets import SDFSamples\n",
        "from NSM.mesh import create_mesh\n",
        "\n",
        "# For working with data\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Plot pyvista mesh interactively using plotly\n",
        "def pv_to_plotly(mesh, color=\"deepskyblue\", opacity=1.0):\n",
        "    mesh = mesh.extract_surface().triangulate()\n",
        "    faces = mesh.faces.reshape(-1, 4)\n",
        "    return go.Mesh3d(x=mesh.points[:, 0], y=mesh.points[:, 1], z=mesh.points[:, 2],\n",
        "                    i=faces[:, 1], j=faces[:, 2], k=faces[:, 3],\n",
        "                    color=color, opacity=opacity, flatshading=False,\n",
        "                    lighting=dict(ambient=0.12, diffuse=0.88, specular=0.05,\n",
        "                                  roughness=0.9, fresnel=0.0),\n",
        "                    lightposition=dict(x=0, y=0, z=2))\n",
        "\n",
        "# Plot pyvista pointcloud interactively using plotly\n",
        "def pv_points_to_plotly(mesh, color='red', size=4):\n",
        "    pts = mesh.points\n",
        "    return go.Scatter3d(x=pts[:, 0], y=pts[:, 1], z=pts[:, 2],\n",
        "                        mode='markers',\n",
        "                        marker=dict(size=size, color=color, opacity=1.0))\n",
        "\n",
        "# Monkey patch for data types ----\n",
        "from NSM.helper_funcs import safe_load_mesh_scalars, fixed_point_coords\n",
        "meshes.Mesh.load_mesh_scalars = safe_load_mesh_scalars\n",
        "meshes.Mesh.point_coords = property(fixed_point_coords)\n",
        "\n",
        "import pymskt.mesh.meshTools as meshTools\n",
        "_original_signed_distance_to_mesh = meshTools.pcu.signed_distance_to_mesh\n",
        "def _signed_distance_to_mesh_patch(pts, points, faces):\n",
        "    pts = np.asarray(pts, dtype=np.float64)     # force double precision\n",
        "    points = np.asarray(points, dtype=np.float64)\n",
        "    faces = np.asarray(faces, dtype=np.int32)   # ensure integer type for faces\n",
        "    return _original_signed_distance_to_mesh(pts, points, faces)\n",
        "meshTools.pcu.signed_distance_to_mesh = _signed_distance_to_mesh_patch\n",
        "# End monkey patch ----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4SbC6Jhgdx"
      },
      "source": [
        "# 2. Load in models and data\n",
        "---\n",
        "Load models and data to do demo shape completion in your Colab runtime environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HEZfqG6hEVN"
      },
      "source": [
        "## Download models and meshes to appropriate folders\n",
        "Use gdown syntax to download demo model and data files to their respective directories. To customize, adjust paths using form fields on right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30csw-ujwA8N"
      },
      "outputs": [],
      "source": [
        "# Download models and meshes to appropriate folders\n",
        "\n",
        "# Update these paths to point to your model and data\n",
        "MODEL_DIR = \"run_v44\" # @param [\"run_v44\"] {\"allow-input\":true}\n",
        "!gdown 1hRLyVdtqD2tF6wbE5m1Da0hLtHXiQ_oj\n",
        "!unzip -o {MODEL_DIR}.zip -d {MODEL_DIR} && rm -f {MODEL_DIR}.zip\n",
        "\n",
        "# Checkpoint to use\n",
        "CKPT = \"3000\" # @param [\"3000\"] {\"allow-input\":true}\n",
        "CKPT_fn = CKPT + '.pth'\n",
        "\n",
        "# Fossil directory\n",
        "fossil_dir = \"fossils\" # @param [\"fossils\"] {\"allow-input\":true}\n",
        "os.makedirs(fossil_dir, exist_ok=True)\n",
        "%cd $fossil_dir\n",
        "!gdown 15c9e_LNPlWfIHXa3EcBR0fWHvdOSjiSl\n",
        "\n",
        "# Modern vertebrae directory\n",
        "vertebrae_dir = \"vertebrae_meshes\" # @param [\"vertebrae_meshes\"] {\"allow-input\":true}\n",
        "%cd $wd\n",
        "!rm -rf $vertebrae_dir # Delete demo vertebrae_meshes dir from nsm github\n",
        "!gdown 1EaQJEfryoziFjdfYmI2-UPoF0wvhdnhS\n",
        "!unzip -o {vertebrae_dir}.zip -d {vertebrae_dir} && rm -f {vertebrae_dir}.zip\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"shape_completion/predictions\" # @param [\"outputs\"] {\"allow-input\":true}\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\n\\033[92mSet up working directory and downloaded model and mesh files.\")\n",
        "print(f\"Model directory: {MODEL_DIR}\")\n",
        "print(f\"Checkpoint: {CKPT}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYEQCEYJwA8P"
      },
      "source": [
        "# 3. Prepare models and data for inference\n",
        "---\n",
        "\n",
        "Prepare models and data to run inference (complete the shape from a partial mesh (modern or fossil)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3yfwVjih1bm"
      },
      "source": [
        "## Load model and latent codes\n",
        "Load files that were downloaded in **2. Load models and data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RxqD8QMwA8O"
      },
      "outputs": [],
      "source": [
        "# Load model and latent codes\n",
        "\n",
        "# Change to model directory\n",
        "%cd $MODEL_DIR\n",
        "\n",
        "# Load config\n",
        "config = load_config(config_path='model_params_config.json')\n",
        "device = config.get(\"device\", \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths to model and latent codes\n",
        "LC_PATH = f'latent_codes/{CKPT}.pth'\n",
        "MODEL_PATH = f'model/{CKPT}.pth'\n",
        "\n",
        "# Load model and latents\n",
        "print(\"Loading model and latents...\")\n",
        "model, latent_ckpt, latent_codes = load_model_and_latents(MODEL_PATH, LC_PATH, config, device)\n",
        "\n",
        "# Compute statistics\n",
        "mean_latent = latent_codes.mean(dim=0, keepdim=True)\n",
        "latent_std = latent_codes.std().mean()\n",
        "_, top_k_reg = get_top_k_pcs(latent_codes, threshold=0.99)\n",
        "\n",
        "# Return to original directory\n",
        "%cd $wd\n",
        "\n",
        "print(f\"\\nLatent size: {config['latent_size']}\")\n",
        "print(f\"Number of training samples: {len(latent_codes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uyr5U5KiH2p"
      },
      "source": [
        "## Load mesh into latent space\n",
        "Pick whether to load a modern (vertebrae_dir) or fossil (fossil_dir) vertebrae from the demo directories, then randomly select a file from the directory for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMndJ7XTiGXY"
      },
      "outputs": [],
      "source": [
        "# Load mesh into latent space\n",
        "\n",
        "# Pick a mesh\n",
        "mesh_dir = fossil_dir # @param [\"fossil_dir\",\"vertebrae_dir\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "mesh_path = random.choice(os.listdir(mesh_dir))\n",
        "print(f\"Mesh being loaded from directory: {mesh_dir}\\n{mesh_path}\\n\")\n",
        "\n",
        "# Setup output directory\n",
        "mesh_name = os.path.splitext(os.path.basename(mesh_path))[0]\n",
        "outfpath = os.path.join(OUTPUT_DIR, mesh_name)\n",
        "os.makedirs(outfpath, exist_ok=True)\n",
        "print(f\"Saving results to output directory: {outfpath}\")\n",
        "\n",
        "# Convert PLY to VTK if needed\n",
        "mesh_path = os.path.join(mesh_dir, mesh_path)\n",
        "vert_fname = mesh_path\n",
        "if '.ply' in mesh_path.lower():\n",
        "    print(\"Converting PLY to VTK...\")\n",
        "    mesh, vert_fname = convert_ply_to_vtk(mesh_path, save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPghx6uVihFl"
      },
      "source": [
        "## Set up the SDF dataset\n",
        "Load mesh into the trained model using SDFSamples() using the same preprocessing that was used during model training (parameters are retrieved from the model config file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIgtDyVmievZ"
      },
      "outputs": [],
      "source": [
        "# Set up the SDF dataset\n",
        "\n",
        "# Setup dataset\n",
        "print(\"\\n-----Setting up dataset-----\")\n",
        "sdf_dataset = SDFSamples(\n",
        "    list_mesh_paths=[vert_fname],\n",
        "    multiprocessing=False,\n",
        "    subsample=config[\"samples_per_object_per_batch\"],\n",
        "    print_filename=True,\n",
        "    n_pts=config[\"n_pts_per_object\"],\n",
        "    p_near_surface=config['percent_near_surface'],\n",
        "    p_further_from_surface=config['percent_further_from_surface'],\n",
        "    sigma_near=config['sigma_near'],\n",
        "    sigma_far=config['sigma_far'],\n",
        "    rand_function=config['random_function'],\n",
        "    center_pts=config['center_pts'],\n",
        "    norm_pts=config['normalize_pts'],\n",
        "    scale_method=config['scale_method'],\n",
        "    reference_mesh=None,\n",
        "    verbose=config['verbose'],\n",
        "    save_cache=config['cache'],\n",
        "    equal_pos_neg=config['equal_pos_neg'],\n",
        "    fix_mesh=config['fix_mesh'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-fcsG5jrQD"
      },
      "source": [
        "## Prepare partial pointcloud for shape completion\n",
        "First, convert the mesh to a pointcloud. Then, downsample it to capture sufficient detail and minimal noise (n_pts=180). Finally, adjust sampled points so a percentage of them are near, far, and on the surface, meaning they will have a variety of SDF values, allowing the trained model to find a smooth surface between the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MEfiLsewA8P"
      },
      "outputs": [],
      "source": [
        "# Prepare partial pointcloud for shape completion\n",
        "\n",
        "# Downsample partial pointcloud\n",
        "print(\"\\n-----Preparing partial pointcloud-----\")\n",
        "n_pts = 180 # @param {\"type\":\"slider\",\"min\":100,\"max\":800,\"step\":20}\n",
        "partial_pts = downsample_partial_pointcloud(vert_fname, n_pts)\n",
        "partial_pts = torch.tensor(partial_pts, dtype=torch.float32)\n",
        "partial_cloud = pv.PolyData(partial_pts.cpu().numpy())\n",
        "partial_cloud.save(os.path.join(outfpath, f\"{mesh_name}_partial_input.vtk\"))\n",
        "\n",
        "# Sample points with SDF values\n",
        "partial_pts, sdfs = sample_near_surface(\n",
        "    partial_pts, eps=0.005, fraction_nonzero=0.4,\n",
        "    fraction_far=0.05, far_eps=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xZ-c_ZHmE3E"
      },
      "source": [
        "# 4. Shape completion\n",
        "---\n",
        "Optimize partial point cloud latents and complete the shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69Oc6Y_kkRs"
      },
      "source": [
        "## Optimize latents\n",
        "Since NSM is an auto-decoder framework, novel data cannot be encoded into the model the exact same was as training data. To encode new data into the model's latent space, optimization is required, which is similar to a mini training session. After several tests using chamfer distance on our training and test data as ground truths, we determined a 2 phase optimization technique with the defined parameters. Every fossil and dataset are different, and they can be adjusted to fit your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B96Qx7K0kjQd"
      },
      "outputs": [],
      "source": [
        "# Optimize latents\n",
        "print(\"\\n\\033[33m-----This will take ~6 minutes----\\033[0m\")\n",
        "print(\"\\n-----Optimizing latents (Phase 1: Coarse)-----\")\n",
        "latent_partial, _ = optimize_latent_partial(\n",
        "    model, partial_pts, sdfs, config['latent_size'],\n",
        "    mean_latent=mean_latent, latent_init=latent_codes, top_k=top_k_reg,\n",
        "    iters=5000, lr=1e-4, lambda_reg=1e-3, clamp_val=2.0,\n",
        "    latent_std=latent_std, scheduler_step=800, scheduler_gamma=0.8,\n",
        "    batch_inference_size=32768, multi_stage=False, device=device)\n",
        "\n",
        "print(\"\\n-----Optimizing latents (Phase 2: Refinement)-----\")\n",
        "latent_partial, _ = optimize_latent_partial(\n",
        "    model, partial_pts, sdfs, config['latent_size'],\n",
        "    latent_init=latent_partial, top_k=top_k_reg,\n",
        "    iters=8000, lr=1.3e-5, lambda_reg=7e-5, clamp_val=None,\n",
        "    latent_std=latent_std, scheduler_step=800, scheduler_gamma=0.7,\n",
        "    batch_inference_size=32768, multi_stage=True, device=device)\n",
        "\n",
        "print(\"\\n\\033[92mLatent optimization complete\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVR9ABWmlExA"
      },
      "source": [
        "## Complete mesh from optimized latent\n",
        "Feed optimized latent into trained model and reconstruct it into a completed mesh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBH_4cOXwA8Q"
      },
      "outputs": [],
      "source": [
        "# Complete mesh from optimized latent\n",
        "\n",
        "# Reconstruction parameters\n",
        "recon_grid_origin = 1.0\n",
        "n_pts_per_axis = 256 # @param [\"256\",\"128\",\"384\"] {\"type\":\"raw\"}\n",
        "voxel_origin = (-recon_grid_origin, -recon_grid_origin, -recon_grid_origin)\n",
        "voxel_size = (recon_grid_origin * 2) / (n_pts_per_axis - 1)\n",
        "offset = np.array([0.0, 0.0, 0.0])\n",
        "scale = 1.0\n",
        "icp_transform = NumpyTransform(np.eye(4))\n",
        "objects = 1\n",
        "\n",
        "# Reconstruct mesh\n",
        "print(\"\\n\\033[93m-----Reconstructing mesh-----\\033[0m\")\n",
        "with torch.no_grad():\n",
        "    mesh_out = create_mesh(\n",
        "        decoder=model, latent_vector=latent_partial,\n",
        "        n_pts_per_axis=n_pts_per_axis,\n",
        "        voxel_origin=voxel_origin, voxel_size=voxel_size,\n",
        "        path_original_mesh=None,\n",
        "        offset=offset, scale=scale, icp_transform=icp_transform,\n",
        "        objects=objects, verbose=True, device=device,\n",
        "        smooth=1.0, scale_to_original_mesh=False)\n",
        "\n",
        "# Ensure it's PyVista PolyData\n",
        "if isinstance(mesh_out, list):\n",
        "    mesh_out = mesh_out[0]\n",
        "if not isinstance(mesh_out, pv.PolyData):\n",
        "    mesh_pv = mesh_out.extract_geometry()\n",
        "else:\n",
        "    mesh_pv = mesh_out\n",
        "\n",
        "# Clean and triangulate\n",
        "mesh_pv = mesh_pv.clean()\n",
        "mesh_pv = mesh_pv.triangulate()\n",
        "\n",
        "# Save mesh\n",
        "output_path = os.path.join(outfpath, f\"{mesh_name}_shape_completion.vtk\")\n",
        "color = np.array([112, 215, 222], dtype=np.uint8)  # RGB color\n",
        "rgb = np.tile(color, (mesh_pv.n_points, 1))\n",
        "mesh_pv.point_data.clear()\n",
        "mesh_pv.point_data['Colors'] = rgb\n",
        "mesh_pv.save(output_path)\n",
        "\n",
        "print(f\"\\n\\033[92mCompleted mesh saved to: {output_path}\\033[0m\")\n",
        "print(f\"Number of points: {mesh_pv.n_points}\")\n",
        "print(f\"Number of faces: {mesh_pv.n_faces_strict}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1HbsDjwA8Q"
      },
      "source": [
        "# 5. Inspect Results\n",
        "---\n",
        "Plot the original versus shape completed mesh. Rerun and adjust optimization parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZmQCtgL3Mu0W"
      },
      "outputs": [],
      "source": [
        "#@title Plot the original mesh\n",
        "\n",
        "# Read mesh\n",
        "original_mesh = pv.read(os.path.join(mesh_dir, f\"{mesh_name}.vtk\"))\n",
        "original_mesh.compute_normals(inplace=True)\n",
        "\n",
        "# Plot figure\n",
        "fig = go.Figure()\n",
        "trace = pv_to_plotly(original_mesh, 'goldenrod', 1)\n",
        "trace.name = \"Original mesh\"\n",
        "fig.add_trace(trace)\n",
        "for trace in fig.data:\n",
        "    trace.showlegend = True\n",
        "fig.update_layout(title=dict(text=f\"Original Mesh (before completion)<br>{mesh_name}\",\n",
        "                             x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"),\n",
        "                  showlegend=True,\n",
        "                  scene_aspectmode='data',\n",
        "                  legend=dict(x=1.02, y=1, bgcolor=\"rgba(255,255,255,0.7)\",\n",
        "                              bordercolor=\"black\", borderwidth=1),\n",
        "                  margin=dict(l=10, r=10, b=10, t=80))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hvQPW64n5p-z"
      },
      "outputs": [],
      "source": [
        "#@title Plot the completed mesh vs sampled point cloud\n",
        "\n",
        "# Read mesh\n",
        "partial_mesh = pv.read(os.path.join(outfpath, f\"{mesh_name}_partial_input.vtk\"))\n",
        "completed_mesh = pv.read(output_path)\n",
        "\n",
        "# Plot figure\n",
        "fig = go.Figure()\n",
        "trace = (pv_to_plotly(completed_mesh, 'deepskyblue', 1)) # Completed surface\n",
        "trace.name = \"Completed mesh\"\n",
        "fig.add_trace(trace)\n",
        "trace = (pv_points_to_plotly(partial_mesh, 'darkseagreen', size=3)) # Partial point cloud\n",
        "trace.name = \"Partial point cloud\"\n",
        "fig.add_trace(trace)\n",
        "for trace in fig.data:\n",
        "    trace.showlegend = True\n",
        "fig.update_layout(title=dict(text=f\"Completed mesh<br>{mesh_name}\",\n",
        "                             x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"),\n",
        "                  showlegend=True,\n",
        "                  scene_aspectmode='data',\n",
        "                  legend=dict(x=1.02, y=1, bgcolor=\"rgba(255,255,255,0.7)\",\n",
        "                              bordercolor=\"black\", borderwidth=1),\n",
        "                  margin=dict(l=10, r=10, b=10, t=80))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA8TFGIpl3FO"
      },
      "source": [
        "# Optional: Save a spinning gif of your 3D models\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H7Ruqh3l8Pn"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Read mesh\n",
        "mesh_type = \"original\" # @param [\"original\",\"partial\"] {\"allow-input\":true}\n",
        "if mesh_type == \"partial\":\n",
        "    mesh = partial_mesh\n",
        "else:\n",
        "    mesh = original_mesh\n",
        "\n",
        "# Create plotter\n",
        "plotter = pv.Plotter(off_screen=True, window_size=(800, 800))\n",
        "plotter.set_background(\"white\")\n",
        "plotter.add_mesh(mesh, color=\"deepskyblue\", opacity=1.0)\n",
        "\n",
        "# Open GIF\n",
        "gif_path = f\"{mesh_name}_{mesh_type}_spin.gif\"\n",
        "plotter.open_gif(gif_path, fps=20)\n",
        "\n",
        "# Camera setup\n",
        "plotter.camera.zoom(1.2)\n",
        "\n",
        "# Rotate + write frames\n",
        "n_frames = 120 # @param {\"type\":\"slider\",\"min\":120,\"max\":360,\"step\":120}\n",
        "for _ in range(n_frames):\n",
        "    plotter.camera.Azimuth(360 / n_frames)\n",
        "    plotter.write_frame()\n",
        "plotter.close()\n",
        "\n",
        "print(f\"Saved GIF to {gif_path}\")\n",
        "\n",
        "# Download to your local machine\n",
        "files.download(gif_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/nsm/blob/main/demos/shape_completion_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuJS_OF3wA8K"
      },
      "source": [
        "#**Squamate Vertebra Shape Completion Demo**   \n",
        "*Last edited 19 Jan 2026*\n",
        "\n",
        "\n",
        "This notebook demonstrates shape completion for partial vertebrae (modern and fossil) using a trained Neural Shape Model (NSM; Gatti et al. 2025, Park et al. 2019). It can be run fully in demo mode without connecting to your Google Drive. Adjust parameters using form fields and make sure your runtime environment is set to run on GPU. Full repository code is available at [aubricot/nsm on GitHub](https://github.com/aubricot/nsm).\n",
        "\n",
        "Modern vertebra meshes are derived from micro-CT data produced by the oVert Initiative (Blackburn et al. 2024). Fossil vertebra were downloaded from MorphoSource ([UF546657](https://doi.org/10.17602/M2/M600663); [UF271967](https://n2t.net/ark:/87602/m4/M69199)). All vertebrae were aligned and scaled using ATLAS before training (Porto et al. 2026).\n",
        "\n",
        "\n",
        "**References**\n",
        "* Blackburn et al. 2024, BioScience. https://doi.org/10.1093/biosci/biad120\n",
        "* Gatti et al. 2025, IEEE TMI. https://doi.org/10.1109/tmi.2024.3485613\n",
        "* Park et al. 2019, CVPR. https://doi.org/10.48550/arXiv.1901.05103\n",
        "* Porto et al. 2026, in prep. https://github.com/agporto/ATLAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arE-9o6jwA8L"
      },
      "source": [
        "## 1. Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU and CUDA info - make sure Colab Runtime set to GPU\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# Check GPU and CUDA\n",
        "!nvcc --version\n",
        "gpu = !nvidia-smi\n",
        "gpu = '\\n'.join(gpu)\n",
        "print('\\033[91mNot connected to a GPU\\033[0m' if 'failed' in gpu else gpu)\n",
        "\n",
        "# Check RAM\n",
        "ram = virtual_memory().total / 1e9\n",
        "print(f'\\033[92mYour runtime has {ram:.1f} GB of RAM\\033[0m\\n')"
      ],
      "metadata": {
        "id": "IX_Hd2ZXibqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose where to save results\n",
        "\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image tagging file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "base_wd = \"/content/drive/MyDrive\" # @param [\"/content/drive/MyDrive/nsm\"] {\"allow-input\":true}\n",
        "wd = base_wd + \"/nsm\"\n",
        "print(f\"\\033[92mWorking directory set to: \\n{wd}\\033[0m\")"
      ],
      "metadata": {
        "id": "ZP-qynnLiqNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up environment and install NSM\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Install PyTorch with CUDA support (Colab typically has CUDA 11.8 or 12.x)\n",
        "print(\"\\033[92mSetting up environment...\\033[0m\")\n",
        "print(\"\\n\\033[33m-----This will take a few minutes----\\033[0m\")\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install pyvista mskt open3d scikit-learn matplotlib pandas numpy scipy\n",
        "!pip install ipywidgets\n",
        "!pip install nibabel scikit-image opencv-python open3d\n",
        "\n",
        "# Clone NSM repository\n",
        "if not os.path.exists(wd):\n",
        "    print(\"Cloning NSM repository...\")\n",
        "    os.makedirs(base_wd, exist_ok=True)\n",
        "    %cd $base_wd\n",
        "    !git clone https://github.com/aubricot/nsm.git\n",
        "else:\n",
        "    print(\"NSM directory already exists\")\n",
        "\n",
        "# Navigate to nsm directory and install\n",
        "%cd $wd\n",
        "\n",
        "# Install requirements\n",
        "print(\"\\n-----Installing requirements-----\")\n",
        "!python -m pip install -r requirements.txt\n",
        "\n",
        "# Install NSM package\n",
        "print(\"\\n-----Installing NSM-----\")\n",
        "!pip install .\n",
        "\n",
        "# Add to Python path\n",
        "sys.path.insert(0, wd)\n",
        "%cd $wd\n",
        "print(f\"\\n\\033[92mCurrent working directory set to: {os.getcwd()}\\033[0m\")"
      ],
      "metadata": {
        "id": "jHhleWhCtA1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries and define functions\n",
        "\n",
        "# For rendering meshes\n",
        "import pyvista as pv\n",
        "pv.start_xvfb() # Enable PyVista for Colab\n",
        "import plotly.graph_objects as go\n",
        "import pymskt.mesh.meshes as meshes\n",
        "\n",
        "# For working with ML\n",
        "import torch\n",
        "from NSM.helper_funcs import load_config, load_model_and_latents\n",
        "from NSM.optimization import get_top_k_pcs\n",
        "from NSM.helper_funcs import NumpyTransform, convert_ply_to_vtk\n",
        "from NSM.optimization import (sample_near_surface,\n",
        "    downsample_partial_pointcloud,\n",
        "    optimize_latent_partial)\n",
        "from NSM.datasets import SDFSamples\n",
        "from NSM.mesh import create_mesh\n",
        "\n",
        "# For working with data\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Plot pyvista mesh interactively using plotly\n",
        "def pv_to_plotly(mesh, color=\"deepskyblue\", opacity=1.0):\n",
        "    mesh = mesh.extract_surface().triangulate()\n",
        "    faces = mesh.faces.reshape(-1, 4)\n",
        "    return go.Mesh3d(x=mesh.points[:, 0], y=mesh.points[:, 1], z=mesh.points[:, 2],\n",
        "                    i=faces[:, 1], j=faces[:, 2], k=faces[:, 3],\n",
        "                    color=color, opacity=opacity, flatshading=False,\n",
        "                    lighting=dict(ambient=0.12, diffuse=0.88, specular=0.05,\n",
        "                                  roughness=0.9, fresnel=0.0),\n",
        "                    lightposition=dict(x=0, y=0, z=2))\n",
        "\n",
        "# Plot pyvista pointcloud interactively using plotly\n",
        "def pv_points_to_plotly(mesh, color='red', size=4):\n",
        "    pts = mesh.points\n",
        "    return go.Scatter3d(x=pts[:, 0], y=pts[:, 1], z=pts[:, 2],\n",
        "                        mode='markers',\n",
        "                        marker=dict(size=size, color=color, opacity=1.0))\n",
        "\n",
        "# Monkey patch for data types ----\n",
        "from NSM.helper_funcs import safe_load_mesh_scalars, fixed_point_coords\n",
        "meshes.Mesh.load_mesh_scalars = safe_load_mesh_scalars\n",
        "meshes.Mesh.point_coords = property(fixed_point_coords)\n",
        "\n",
        "import pymskt.mesh.meshTools as meshTools\n",
        "_original_signed_distance_to_mesh = meshTools.pcu.signed_distance_to_mesh\n",
        "def _signed_distance_to_mesh_patch(pts, points, faces):\n",
        "    pts = np.asarray(pts, dtype=np.float64)     # force double precision\n",
        "    points = np.asarray(points, dtype=np.float64)\n",
        "    faces = np.asarray(faces, dtype=np.int32)   # ensure integer type for faces\n",
        "    return _original_signed_distance_to_mesh(pts, points, faces)\n",
        "meshTools.pcu.signed_distance_to_mesh = _signed_distance_to_mesh_patch\n",
        "# End monkey patch ----"
      ],
      "metadata": {
        "id": "Y7teNyyTwCfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30csw-ujwA8N"
      },
      "outputs": [],
      "source": [
        "#@title Download models and meshes to appropriate folders\n",
        "\n",
        "# Update these paths to point to your model and data\n",
        "MODEL_DIR = \"run_v44\" # @param [\"run_v44\"] {\"allow-input\":true}\n",
        "!gdown 1hRLyVdtqD2tF6wbE5m1Da0hLtHXiQ_oj\n",
        "!unzip -o {MODEL_DIR}.zip -d {MODEL_DIR} && rm -f {MODEL_DIR}.zip\n",
        "\n",
        "# Checkpoint to use\n",
        "CKPT = \"3000\" # @param [\"3000\"] {\"allow-input\":true}\n",
        "CKPT_fn = CKPT + '.pth'\n",
        "\n",
        "# Fossil directory\n",
        "fossil_dir = \"fossils\" # @param [\"fossils\"] {\"allow-input\":true}\n",
        "os.makedirs(fossil_dir, exist_ok=True)\n",
        "%cd $fossil_dir\n",
        "!gdown 15c9e_LNPlWfIHXa3EcBR0fWHvdOSjiSl\n",
        "\n",
        "# Modern vertebrae directory\n",
        "vertebrae_dir = \"vertebrae_meshes\" # @param [\"vertebrae_meshes\"] {\"allow-input\":true}\n",
        "%cd $wd\n",
        "!rm -rf $vertebrae_dir # Delete demo vertebrae_meshes dir from nsm github\n",
        "!gdown 1EaQJEfryoziFjdfYmI2-UPoF0wvhdnhS\n",
        "!unzip -o {vertebrae_dir}.zip -d {vertebrae_dir} && rm -f {vertebrae_dir}.zip\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"shape_completion/predictions\" # @param [\"outputs\"] {\"allow-input\":true}\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\n\\033[92mSet up working directory and downloaded model and mesh files.\")\n",
        "print(f\"Model directory: {MODEL_DIR}\")\n",
        "print(f\"Checkpoint: {CKPT}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYEQCEYJwA8P"
      },
      "source": [
        "## 2. Shape Completion\n",
        "\n",
        "Complete the shape from a partial mesh (modern or fossil).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RxqD8QMwA8O"
      },
      "outputs": [],
      "source": [
        "#@title Load model and latent codes\n",
        "\n",
        "# Change to model directory\n",
        "%cd $MODEL_DIR\n",
        "\n",
        "# Load config\n",
        "config = load_config(config_path='model_params_config.json')\n",
        "device = config.get(\"device\", \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths to model and latent codes\n",
        "LC_PATH = f'latent_codes/{CKPT}.pth'\n",
        "MODEL_PATH = f'model/{CKPT}.pth'\n",
        "\n",
        "# Load model and latents\n",
        "print(\"Loading model and latents...\")\n",
        "model, latent_ckpt, latent_codes = load_model_and_latents(MODEL_PATH, LC_PATH, config, device)\n",
        "\n",
        "# Compute statistics\n",
        "mean_latent = latent_codes.mean(dim=0, keepdim=True)\n",
        "latent_std = latent_codes.std().mean()\n",
        "_, top_k_reg = get_top_k_pcs(latent_codes, threshold=0.99)\n",
        "\n",
        "# Return to original directory\n",
        "%cd $wd\n",
        "\n",
        "print(f\"\\nLatent size: {config['latent_size']}\")\n",
        "print(f\"Number of training samples: {len(latent_codes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MEfiLsewA8P"
      },
      "outputs": [],
      "source": [
        "#@title Load mesh into latent space\n",
        "\n",
        "# Pick a mesh\n",
        "mesh_dir = fossil_dir # @param [\"fossil_dir\",\"vertebrae_dir\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "mesh_path = random.choice(os.listdir(mesh_dir))\n",
        "print(f\"Mesh being loaded from directory: {mesh_dir}\\n{mesh_path}\\n\")\n",
        "\n",
        "# Setup output directory\n",
        "mesh_name = os.path.splitext(os.path.basename(mesh_path))[0]\n",
        "outfpath = os.path.join(OUTPUT_DIR, mesh_name)\n",
        "os.makedirs(outfpath, exist_ok=True)\n",
        "print(f\"Saving results to output directory: {outfpath}\")\n",
        "\n",
        "# Convert PLY to VTK if needed\n",
        "mesh_path = os.path.join(mesh_dir, mesh_path)\n",
        "vert_fname = mesh_path\n",
        "if '.ply' in mesh_path.lower():\n",
        "    print(\"Converting PLY to VTK...\")\n",
        "    mesh, vert_fname = convert_ply_to_vtk(mesh_path, save=True)\n",
        "\n",
        "# Setup dataset\n",
        "print(\"\\n-----Setting up dataset-----\")\n",
        "sdf_dataset = SDFSamples(\n",
        "    list_mesh_paths=[vert_fname],\n",
        "    multiprocessing=False,\n",
        "    subsample=config[\"samples_per_object_per_batch\"],\n",
        "    print_filename=True,\n",
        "    n_pts=config[\"n_pts_per_object\"],\n",
        "    p_near_surface=config['percent_near_surface'],\n",
        "    p_further_from_surface=config['percent_further_from_surface'],\n",
        "    sigma_near=config['sigma_near'],\n",
        "    sigma_far=config['sigma_far'],\n",
        "    rand_function=config['random_function'],\n",
        "    center_pts=config['center_pts'],\n",
        "    norm_pts=config['normalize_pts'],\n",
        "    scale_method=config['scale_method'],\n",
        "    reference_mesh=None,\n",
        "    verbose=config['verbose'],\n",
        "    save_cache=config['cache'],\n",
        "    equal_pos_neg=config['equal_pos_neg'],\n",
        "    fix_mesh=config['fix_mesh'])\n",
        "\n",
        "# Get SDF data\n",
        "sdf_sample = sdf_dataset[0]\n",
        "sample_dict, _ = sdf_sample\n",
        "points = sample_dict['xyz'].to(device)\n",
        "sdf_vals = sample_dict['gt_sdf']\n",
        "\n",
        "# Downsample partial pointcloud\n",
        "print(\"\\n-----Preparing partial pointcloud-----\")\n",
        "partial_pts = downsample_partial_pointcloud(vert_fname, 180)\n",
        "partial_pts = torch.tensor(partial_pts, dtype=torch.float32)\n",
        "partial_cloud = pv.PolyData(partial_pts.cpu().numpy())\n",
        "partial_cloud.save(os.path.join(outfpath, f\"{mesh_name}_partial_input.vtk\"))\n",
        "\n",
        "# Sample points with SDF values\n",
        "partial_pts, sdfs = sample_near_surface(\n",
        "    partial_pts, eps=0.005, fraction_nonzero=0.4,\n",
        "    fraction_far=0.05, far_eps=0.05)\n",
        "\n",
        "# Optimize latents\n",
        "print(\"\\n-----Optimizing latents (Phase 1: Coarse)-----\")\n",
        "latent_partial, _ = optimize_latent_partial(\n",
        "    model, partial_pts, sdfs, config['latent_size'],\n",
        "    mean_latent=mean_latent, latent_init=latent_codes, top_k=top_k_reg,\n",
        "    iters=5000, lr=1e-4, lambda_reg=1e-3, clamp_val=2.0,\n",
        "    latent_std=latent_std, scheduler_step=800, scheduler_gamma=0.8,\n",
        "    batch_inference_size=32768, multi_stage=False, device=device)\n",
        "\n",
        "print(\"\\n-----Optimizing latents (Phase 2: Refinement)-----\")\n",
        "latent_partial, _ = optimize_latent_partial(\n",
        "    model, partial_pts, sdfs, config['latent_size'],\n",
        "    latent_init=latent_partial, top_k=top_k_reg,\n",
        "    iters=8000, lr=1.3e-5, lambda_reg=7e-5, clamp_val=None,\n",
        "    latent_std=latent_std, scheduler_step=800, scheduler_gamma=0.7,\n",
        "    batch_inference_size=32768, multi_stage=True, device=device)\n",
        "\n",
        "print(\"\\n\\033[92mLatent optimization complete\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBH_4cOXwA8Q"
      },
      "outputs": [],
      "source": [
        "#@title Complete and reconstruct mesh\n",
        "\n",
        "# Reconstruction parameters\n",
        "recon_grid_origin = 1.0\n",
        "n_pts_per_axis = 256 # @param [\"256\",\"128\",\"384\"] {\"type\":\"raw\"}\n",
        "voxel_origin = (-recon_grid_origin, -recon_grid_origin, -recon_grid_origin)\n",
        "voxel_size = (recon_grid_origin * 2) / (n_pts_per_axis - 1)\n",
        "offset = np.array([0.0, 0.0, 0.0])\n",
        "scale = 1.0\n",
        "icp_transform = NumpyTransform(np.eye(4))\n",
        "objects = 1\n",
        "\n",
        "# Reconstruct mesh\n",
        "print(\"\\n\\033[93m-----Reconstructing mesh-----\\033[0m\")\n",
        "with torch.no_grad():\n",
        "    mesh_out = create_mesh(\n",
        "        decoder=model, latent_vector=latent_partial,\n",
        "        n_pts_per_axis=n_pts_per_axis,\n",
        "        voxel_origin=voxel_origin, voxel_size=voxel_size,\n",
        "        path_original_mesh=None,\n",
        "        offset=offset, scale=scale, icp_transform=icp_transform,\n",
        "        objects=objects, verbose=True, device=device,\n",
        "        smooth=1.0, scale_to_original_mesh=False)\n",
        "\n",
        "# Ensure it's PyVista PolyData\n",
        "if isinstance(mesh_out, list):\n",
        "    mesh_out = mesh_out[0]\n",
        "if not isinstance(mesh_out, pv.PolyData):\n",
        "    mesh_pv = mesh_out.extract_geometry()\n",
        "else:\n",
        "    mesh_pv = mesh_out\n",
        "\n",
        "# Clean and triangulate\n",
        "mesh_pv = mesh_pv.clean()\n",
        "mesh_pv = mesh_pv.triangulate()\n",
        "\n",
        "# Save mesh\n",
        "output_path = os.path.join(outfpath, f\"{mesh_name}_shape_completion.vtk\")\n",
        "color = np.array([112, 215, 222], dtype=np.uint8)  # RGB color\n",
        "rgb = np.tile(color, (mesh_pv.n_points, 1))\n",
        "mesh_pv.point_data.clear()\n",
        "mesh_pv.point_data['Colors'] = rgb\n",
        "mesh_pv.save(output_path)\n",
        "\n",
        "print(f\"\\n\\033[92mCompleted mesh saved to: {output_path}\\033[0m\")\n",
        "print(f\"Number of points: {mesh_pv.n_points}\")\n",
        "print(f\"Number of faces: {mesh_pv.n_faces_strict}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1HbsDjwA8Q"
      },
      "source": [
        "## 3. Inspect Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot the original mesh\n",
        "\n",
        "# Read mesh\n",
        "original_mesh = pv.read(os.path.join(mesh_dir, f\"{mesh_name}.vtk\"))\n",
        "original_mesh.compute_normals(inplace=True)\n",
        "\n",
        "# Plot figure\n",
        "fig = go.Figure()\n",
        "trace = pv_to_plotly(original_mesh, 'goldenrod', 1)\n",
        "trace.name = \"Original mesh\"\n",
        "fig.add_trace(trace)\n",
        "for trace in fig.data:\n",
        "    trace.showlegend = True\n",
        "fig.update_layout(title=dict(text=f\"Original Mesh (before completion)<br>{mesh_name}\",\n",
        "                             x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"),\n",
        "                  showlegend=True,\n",
        "                  scene_aspectmode='data',\n",
        "                  legend=dict(x=1.02, y=1, bgcolor=\"rgba(255,255,255,0.7)\",\n",
        "                              bordercolor=\"black\", borderwidth=1),\n",
        "                  margin=dict(l=10, r=10, b=10, t=80))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZmQCtgL3Mu0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot the completed mesh vs sampled point cloud\n",
        "\n",
        "# Read mesh\n",
        "partial_mesh = pv.read(os.path.join(outfpath, f\"{mesh_name}_partial_input.vtk\"))\n",
        "completed_mesh = pv.read(output_path)\n",
        "\n",
        "# Plot figure\n",
        "fig = go.Figure()\n",
        "trace = (pv_to_plotly(completed_mesh, 'deepskyblue', 1)) # Completed surface\n",
        "trace.name = \"Completed mesh\"\n",
        "fig.add_trace(trace)\n",
        "trace = (pv_points_to_plotly(partial_mesh, 'darkseagreen', size=5)) # Partial point cloud\n",
        "trace.name = \"Partial point cloud\"\n",
        "fig.add_trace(trace)\n",
        "for trace in fig.data:\n",
        "    trace.showlegend = True\n",
        "fig.update_layout(title=dict(text=f\"Completed mesh<br>{mesh_name}\",\n",
        "                             x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"),\n",
        "                  showlegend=True,\n",
        "                  scene_aspectmode='data',\n",
        "                  legend=dict(x=1.02, y=1, bgcolor=\"rgba(255,255,255,0.7)\",\n",
        "                              bordercolor=\"black\", borderwidth=1),\n",
        "                  margin=dict(l=10, r=10, b=10, t=80))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hvQPW64n5p-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}